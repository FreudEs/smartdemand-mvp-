# app.py
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import traceback
from io import BytesIO
from typing import Dict, List, Tuple, Optional, Any

# --- ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ ì„í¬íŠ¸ ---
from data_handler import create_sample_data
from feature_engineer import create_date_features
from model_trainer import MIN_DATA_DAYS
from parallel_processor import run_parallel_processing
from column_mapper import auto_map_columns
from ui_components import (
    apply_custom_css, display_header, display_upload_section,
    display_metrics, display_overall_chart, display_product_analysis,
    display_loading_animation, display_debug_info, display_anomaly_report
)

try:
    from gpt_explainer import get_product_explanation, GPTExplainer
    from report_generator import create_downloadable_report
    ADDITIONAL_FEATURES_ENABLED = True
except ImportError:
    st.error("UI, LLM, PDF ê´€ë ¨ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨. ì¼ë¶€ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    ADDITIONAL_FEATURES_ENABLED = False


# --- í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(page_title="SmartDemand AI", layout="wide", page_icon="ğŸ¯")
apply_custom_css()
display_header()

# --- Session State ì´ˆê¸°í™” ---
if 'data_loaded' not in st.session_state: st.session_state.data_loaded = False
if 'uploaded_file' not in st.session_state: st.session_state.uploaded_file = None
if 'use_sample' not in st.session_state: st.session_state.use_sample = False
if 'gpt_explanations' not in st.session_state: st.session_state.gpt_explanations = {}
if 'api_key' not in st.session_state: st.session_state.api_key = None
if 'font_file' not in st.session_state: st.session_state.font_file = None


# --- ì‚¬ì´ë“œë°” UI ---
with st.sidebar:
    st.markdown("---")
    st.subheader("ğŸ¤– AI í•´ì„¤ ì„¤ì •")
    api_key_input = st.text_input(
        "Fireworks AI API Key", type="password",
        help="AI í•´ì„¤ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ Fireworks AI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    )
    if api_key_input: st.session_state.api_key = api_key_input
    
    st.subheader("ğŸ“„ PDF ì„¤ì •")
    font_file_uploader = st.file_uploader(
        "í•œê¸€ í°íŠ¸ íŒŒì¼ (ì„ íƒ)", type=['ttf'],
        help="PDF ë¦¬í¬íŠ¸ì˜ í•œê¸€ í‘œì‹œë¥¼ ìœ„í•œ í°íŠ¸ íŒŒì¼"
    )
    if font_file_uploader: st.session_state.font_file = font_file_uploader

    st.markdown("---")
    st.subheader("ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì •")
    if st.button("ğŸ”„ ìºì‹œ ì´ˆê¸°í™”", help="ì˜ˆì¸¡ì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‚¬ìš©í•˜ì„¸ìš”"):
        st.cache_data.clear()
        st.cache_resource.clear()
        st.success("âœ… ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.rerun()

# --- ë°ì´í„° ì²˜ë¦¬ ë° ëª¨ë¸ë§ í•¨ìˆ˜ ---
def process_uploaded_file(uploaded_file):
    """
    [ìˆ˜ì •] Streamlitì˜ UploadedFile ê°ì²´ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ BytesIOë¥¼ ì‚¬ìš©í•˜ê³ ,
    ë‹¤ì–‘í•œ ì—‘ì…€ ì—”ì§„ ë° CSV ì¸ì½”ë”©ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤.
    """
    try:
        file_name = uploaded_file.name
        # íŒŒì¼ì„ ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ì½ì–´ BytesIOë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
        file_bytes = BytesIO(uploaded_file.getvalue())

        if file_name.endswith('.csv'):
            try:
                # UTF-8ìœ¼ë¡œ ë¨¼ì € ì‹œë„
                file_bytes.seek(0)
                df = pd.read_csv(file_bytes, encoding='utf-8-sig')
                st.info("âœ… CSV íŒŒì¼ì„ UTF-8 í˜•ì‹ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
            except UnicodeDecodeError:
                # ì‹¤íŒ¨ ì‹œ, UTF-16ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„
                st.warning("âš ï¸ UTF-8ë¡œ íŒŒì¼ì„ ì½ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. UTF-16(ì—‘ì…€ CSV) í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
                file_bytes.seek(0)
                df = pd.read_csv(file_bytes, encoding='utf-16', sep='\t')
                st.info("âœ… CSV íŒŒì¼ì„ UTF-16 í˜•ì‹ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
            return df

        elif file_name.endswith(('.xls', '.xlsx')):
            engines = ['openpyxl', 'xlrd']
            for engine in engines:
                try:
                    file_bytes.seek(0)
                    df = pd.read_excel(file_bytes, engine=engine)
                    st.info(f"âœ… ì—‘ì…€ íŒŒì¼ì„ '{engine}' ì—”ì§„ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
                    return df
                except Exception:
                    st.warning(f"âš ï¸ '{engine}' ì—”ì§„ìœ¼ë¡œ ì—‘ì…€ íŒŒì¼ì„ ì—¬ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë°©ë²•ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")

            # ì—‘ì…€ ì—”ì§„ìœ¼ë¡œ ëª¨ë‘ ì‹¤íŒ¨ ì‹œ, í…ìŠ¤íŠ¸ íŒŒì¼ì¼ ê°€ëŠ¥ì„±ì„ ì—¼ë‘
            st.warning("âš ï¸ ì—‘ì…€ í˜•ì‹ìœ¼ë¡œ íŒŒì¼ì„ ì—¬ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ íŒŒì¼(CSV) í˜•ì‹ìœ¼ë¡œ ë§ˆì§€ë§‰ ì‹œë„ë¥¼ í•©ë‹ˆë‹¤.")
            file_bytes.seek(0)
            df = pd.read_csv(file_bytes, encoding='utf-16', sep='\t')
            st.info("âœ… íŒŒì¼ì„ í…ìŠ¤íŠ¸(UTF-16) í˜•ì‹ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
            return df
        else:
            st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return None
            
    except Exception as e:
        st.error(f"íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.info("ì—‘ì…€ íŒŒì¼(.xls, .xlsx)ì„ ì •ìƒì ìœ¼ë¡œ ì½ê¸° ìœ„í•´ `xlrd`ì™€ `openpyxl` ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (pip install xlrd openpyxl)")
        return None

@st.cache_data
def get_data(uploaded_file, use_sample=False):
    if uploaded_file and not use_sample:
        raw_df = process_uploaded_file(uploaded_file)
        if raw_df is None: return None
    else:
        st.info("ìƒ˜í”Œ ë°ì´í„°ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤...")
        raw_df = create_sample_data()
    
    mapped_df = auto_map_columns(raw_df)
    
    if 'ë‚ ì§œ' not in mapped_df.columns or 'í’ˆëª©ëª…' not in mapped_df.columns or 'íŒë§¤ëŸ‰' not in mapped_df.columns:
        st.error("ìë™ ì»¬ëŸ¼ ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì— 'ë‚ ì§œ', 'í’ˆëª©ëª…', 'íŒë§¤ëŸ‰(ë˜ëŠ” ê°€ê²©)'ì— í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None

    try:
        mapped_df['ë‚ ì§œ'] = pd.to_datetime(mapped_df['ë‚ ì§œ'], errors='coerce')
        if mapped_df['ë‚ ì§œ'].isna().any():
            st.warning("âš ï¸ 'ë‚ ì§œ' ì»¬ëŸ¼ì— ì¸ì‹í•  ìˆ˜ ì—†ëŠ” ê°’ì´ ìˆì–´ í•´ë‹¹ í–‰ì„ ì œì™¸í•©ë‹ˆë‹¤. ë‚ ì§œ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            mapped_df.dropna(subset=['ë‚ ì§œ'], inplace=True)
    except Exception as e:
        st.error(f"'ë‚ ì§œ' ì»¬ëŸ¼ì„ ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

    return create_date_features(mapped_df)


@st.cache_data(ttl=3600)
def load_models_and_ensemble(_df):
    models_data = run_parallel_processing(_df)
    return models_data


# --- ë©”ì¸ ë¡œì§ ---
uploaded_file, upload_btn, sample_btn = display_upload_section()

if upload_btn and uploaded_file:
    st.session_state.uploaded_file = uploaded_file
    st.session_state.use_sample = False
    st.session_state.data_loaded = True
    st.cache_data.clear()
    st.rerun()

if sample_btn:
    st.session_state.uploaded_file = None
    st.session_state.use_sample = True
    st.session_state.data_loaded = True
    st.cache_data.clear()
    st.rerun()

if st.session_state.get('data_loaded', False):
    df = get_data(st.session_state.uploaded_file, st.session_state.use_sample)

    if df is not None:
        loading_placeholder = st.empty()
        with loading_placeholder.container():
            display_loading_animation()
        
        models_data = load_models_and_ensemble(df)
        
        loading_placeholder.empty() 
        
        anomaly_log_df = models_data.pop('total_anomaly_log', pd.DataFrame())
        
        available_products = [p for p, d in models_data.items() if d.get('ensemble_forecast') is not None]

        if available_products:
            display_anomaly_report(anomaly_log_df)

            st.subheader("ğŸ“Š ì „ì²´ ë¹„ì¦ˆë‹ˆìŠ¤ í˜„í™© ìš”ì•½")
            display_metrics(df, models_data)
            display_overall_chart(df, models_data)
            st.markdown("---")

            st.subheader("ğŸ“ˆ í’ˆëª©ë³„ ìƒì„¸ ë¶„ì„ ë° AI ì¸ì‚¬ì´íŠ¸")
            st.markdown("### ğŸ¯ í’ˆëª©ë³„ ì˜ˆì¸¡ ê²°ê³¼")
            for i in range(0, len(available_products), 3):
                cols = st.columns(3)
                for j, col in enumerate(cols):
                    if i + j < len(available_products):
                        product = available_products[i + j]
                        with col:
                            display_product_analysis(product, models_data[product], df)

            if ADDITIONAL_FEATURES_ENABLED:
                st.markdown("---")
                st.subheader("ğŸ“„ ì¶”ê°€ ê¸°ëŠ¥")
                exp_col1, exp_col2 = st.columns(2)

                with exp_col1:
                    with st.expander("ğŸ¤– ëª¨ë“  í’ˆëª© AI í•´ì„¤ ì¼ê´„ ìƒì„±"):
                        if not st.session_state.api_key:
                            st.warning("AI í•´ì„¤ì„ ìƒì„±í•˜ë ¤ë©´ ì‚¬ì´ë“œë°”ì—ì„œ Fireworks AI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                        else:
                            if st.button("ğŸš€ ëª¨ë“  í’ˆëª© í•´ì„¤ ìƒì„± ì‹¤í–‰"):
                                explainer = GPTExplainer(st.session_state.api_key)
                                batch_data = []

                                # [í•µì‹¬ ìˆ˜ì •] AI í•´ì„¤ì— ì´ìƒ í˜„ìƒ ì •ë³´ë¥¼ í¬í•¨ì‹œí‚¤ê¸° ìœ„í•œ ë¡œì§
                                for p, md in models_data.items():
                                    if isinstance(md, dict) and p in available_products:
                                        fc = md.get('ensemble_forecast')
                                        
                                        # í•´ë‹¹ í’ˆëª©ì˜ ì´ìƒ í˜„ìƒ ë¡œê·¸ë§Œ í•„í„°ë§í•˜ê³  ê°€ê³µ
                                        anomaly_info_list = []
                                        if not anomaly_log_df.empty:
                                            product_anomaly_log = anomaly_log_df[anomaly_log_df['í’ˆëª©ëª…'] == p]
                                            if not product_anomaly_log.empty:
                                                # ë„ˆë¬´ ë§ìœ¼ë©´ ìš”ì•½ì´ ì–´ë ¤ìš°ë¯€ë¡œ ìµœê·¼ 5ê°œë§Œ ì‚¬ìš©
                                                for _, row in product_anomaly_log.head(5).iterrows():
                                                    log_str = (
                                                        f"{row['ë‚ ì§œ'].strftime('%Yë…„ %mì›” %dì¼')}: "
                                                        f"íŒë§¤ëŸ‰ì´ ë¹„ì •ìƒì ìœ¼ë¡œ '{row['ì´ìƒ_ìœ í˜•']}'í–ˆìŠµë‹ˆë‹¤. "
                                                        f"(AI ì¶”ì • ì›ì¸: {row['ì¶”ì •_ì›ì¸']})"
                                                    )
                                                    anomaly_info_list.append(log_str)

                                        batch_data.append({
                                            'name': p, 'model': 'Ensemble',
                                            'accuracy': md['ensemble_performance'].get('Accuracy(%)', 0),
                                            'mae': md['ensemble_performance'].get('MAE', 0),
                                            'forecast_7day': fc['ensemble_yhat'].sum() if fc is not None else 0,
                                            'avg_daily': fc['ensemble_yhat'].mean() if fc is not None else 0,
                                            # ê°€ê³µëœ ì´ìƒ í˜„ìƒ ì •ë³´ë¥¼ í•¨ê»˜ ì „ë‹¬
                                            'anomaly_info': anomaly_info_list if anomaly_info_list else None
                                        })

                                with st.spinner("AIê°€ ëª¨ë“  í’ˆëª©ì˜ ë¦¬í¬íŠ¸ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (ê³¼ê±° ë°ì´í„° íŠ¹ì§• í¬í•¨)"):
                                    explanations = explainer.generate_batch_explanations(batch_data)
                                    st.session_state.gpt_explanations.update(explanations)
                                st.success(f"{len(explanations)}ê°œ í’ˆëª©ì˜ í•´ì„¤ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

                with exp_col2:
                    with st.expander("ğŸ“¥ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ"):
                        if st.button("ğŸ“‘ PDF ë¦¬í¬íŠ¸ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ"):
                            with st.spinner("PDF ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘..."):
                                valid_model_data = [md for md in models_data.values() if isinstance(md, dict)]
                                
                                avg_acc = np.mean([md['ensemble_performance'].get('Accuracy(%)',0) for md in valid_model_data if md.get('ensemble_performance')])
                                total_fc = sum([md['ensemble_forecast']['ensemble_yhat'].sum() for md in valid_model_data if md.get('ensemble_forecast') is not None])
                                summary_data = {
                                    'period': f"{df['ë‚ ì§œ'].min().strftime('%Y-%m-%d')} ~ {df['ë‚ ì§œ'].max().strftime('%Y-%m-%d')}",
                                    'product_count': len(available_products), 'avg_accuracy': avg_acc, 'total_forecast': total_fc
                                }
                                products_report_data = {p: {'accuracy': md['ensemble_performance'].get('Accuracy(%)',0),
                                                            'mae': md['ensemble_performance'].get('MAE',0),
                                                            'forecast_7day': md['ensemble_forecast']['ensemble_yhat'].sum() if md.get('ensemble_forecast') is not None else 0,
                                                            'avg_daily': md['ensemble_forecast']['ensemble_yhat'].mean() if md.get('ensemble_forecast') is not None else 0}
                                                      for p, md in models_data.items() if isinstance(md, dict) and p in available_products}
                                
                                font_path = None
                                if st.session_state.font_file:
                                    import tempfile
                                    with tempfile.NamedTemporaryFile(delete=False, suffix='.ttf') as tmp:
                                        tmp.write(st.session_state.font_file.getvalue())
                                        font_path = tmp.name
                                pdf_data = create_downloadable_report(summary_data, products_report_data, st.session_state.gpt_explanations, font_path)
                                st.download_button(
                                    label="ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ", data=pdf_data,
                                    file_name=f"smartdemand_report_{datetime.now().strftime('%Y%m%d')}.pdf",
                                    mime="application/pdf"
                                )

            display_debug_info(df, models_data, MIN_DATA_DAYS)

        st.markdown("---")
        if st.button("ğŸ”„ ìƒˆ ë°ì´í„°ë¡œ ë‹¤ì‹œ ë¶„ì„í•˜ê¸°", use_container_width=True):
            for key in list(st.session_state.keys()): del st.session_state[key]
            st.cache_data.clear()
            st.rerun()
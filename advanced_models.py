# advanced_models.py
import pandas as pd
import numpy as np
from typing import Tuple, Dict, List, Optional, Any
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error
import lightgbm as lgb
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import warnings
warnings.filterwarnings('ignore')

# ë³€ë™ì„±ì´ í° ë°ì´í„°ì— ê°•í•œ ì¶”ê°€ ëª¨ë¸ë“¤

class LSTMPredictor:
    """
    LSTM (Long Short-Term Memory) ëª¨ë¸
    ì¥ì : ì¥ê¸° ì˜ì¡´ì„±ì„ í•™ìŠµí•˜ì—¬ ë³µì¡í•œ ì‹œê³„ì—´ íŒ¨í„´ í¬ì°©
    íŠ¹íˆ ë³€ë™ì„±ì´ í¬ê³  ë¹„ì„ í˜•ì ì¸ íŒ¨í„´ì— ê°•í•¨
    """
    
    def __init__(self, lookback_days: int = 14, forecast_days: int = 7):
        self.lookback_days = lookback_days
        self.forecast_days = forecast_days
        self.model = None
        self.scaler = MinMaxScaler()
        
    def prepare_sequences(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ì‹œê³„ì—´ ë°ì´í„°ë¥¼ LSTM ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜"""
        X, y = [], []
        for i in range(self.lookback_days, len(data) - self.forecast_days + 1):
            X.append(data[i-self.lookback_days:i])
            y.append(data[i:i+self.forecast_days])
        return np.array(X), np.array(y)
    
    def build_model(self, input_shape: Tuple) -> Sequential:
        """LSTM ëª¨ë¸ êµ¬ì¡° ì •ì˜"""
        model = Sequential([
            LSTM(50, activation='relu', return_sequences=True, input_shape=input_shape),
            Dropout(0.2),
            LSTM(50, activation='relu', return_sequences=True),
            Dropout(0.2),
            LSTM(25, activation='relu'),
            Dropout(0.2),
            Dense(self.forecast_days)
        ])
        
        model.compile(optimizer='adam', loss='mse', metrics=['mae'])
        return model
    
    def train(self, df: pd.DataFrame, product_name: str) -> Dict:
        """LSTM ëª¨ë¸ í•™ìŠµ"""
        product_df = df[df['í’ˆëª©ëª…'] == product_name].copy()
        
        if len(product_df) < self.lookback_days + self.forecast_days + 30:
            return {'trained': False, 'message': 'ë°ì´í„° ë¶€ì¡±'}
        
        # ë°ì´í„° ì •ê·œí™”
        sales_data = product_df['íŒë§¤ëŸ‰'].values.reshape(-1, 1)
        scaled_data = self.scaler.fit_transform(sales_data)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y = self.prepare_sequences(scaled_data.flatten())
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        test_size = min(30, int(len(X) * 0.2))
        X_train, X_test = X[:-test_size], X[-test_size:]
        y_train, y_test = y[:-test_size], y[-test_size:]
        
        # ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ
        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
        
        self.model = self.build_model((X_train.shape[1], 1))
        
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        )
        
        history = self.model.fit(
            X_train, y_train,
            epochs=50,
            batch_size=16,
            validation_split=0.1,
            callbacks=[early_stopping],
            verbose=0
        )
        
        # ì„±ëŠ¥ í‰ê°€
        y_pred = self.model.predict(X_test, verbose=0)
        
        # ì—­ì •ê·œí™”
        y_test_original = self.scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
        y_pred_original = self.scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()
        
        mae = mean_absolute_error(y_test_original, y_pred_original)
        mape = np.mean(np.abs((y_test_original - y_pred_original) / np.maximum(y_test_original, 1))) * 100
        
        return {
            'trained': True,
            'performance': {
                'Accuracy(%)': 100 - mape,
                'MAE': mae
            },
            'history': history.history,
            'predictions': y_pred_original
        }
    
    def predict_future(self, df: pd.DataFrame, product_name: str) -> pd.DataFrame:
        """ë¯¸ë˜ 7ì¼ ì˜ˆì¸¡"""
        if self.model is None:
            return pd.DataFrame()
        
        product_df = df[df['í’ˆëª©ëª…'] == product_name].copy()
        
        # ë§ˆì§€ë§‰ lookback_days ë°ì´í„°ë¡œ ì˜ˆì¸¡
        last_sequence = product_df['íŒë§¤ëŸ‰'].values[-self.lookback_days:]
        last_sequence_scaled = self.scaler.transform(last_sequence.reshape(-1, 1)).flatten()
        
        # ì˜ˆì¸¡
        X_future = last_sequence_scaled.reshape((1, self.lookback_days, 1))
        future_pred_scaled = self.model.predict(X_future, verbose=0)
        future_pred = self.scaler.inverse_transform(future_pred_scaled.reshape(-1, 1)).flatten()
        
        # ìŒìˆ˜ ì œê±°
        future_pred = np.maximum(future_pred, 0)
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        last_date = product_df['ë‚ ì§œ'].max()
        future_dates = [last_date + pd.Timedelta(days=i+1) for i in range(self.forecast_days)]
        
        return pd.DataFrame({
            'ë‚ ì§œ': future_dates,
            'íŒë§¤ëŸ‰': future_pred[:self.forecast_days]
        })


class LightGBMPredictor:
    """
    LightGBM ëª¨ë¸
    ì¥ì : XGBoostë³´ë‹¤ ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì , ë²”ì£¼í˜• ë³€ìˆ˜ ìë™ ì²˜ë¦¬
    ë³€ë™ì„±ì´ í° ë°ì´í„°ì—ì„œë„ ì•ˆì •ì ì¸ ì„±ëŠ¥
    """
    
    def __init__(self):
        self.model = None
        self.feature_importance = None
        
    def train(self, features: pd.DataFrame, target: pd.Series) -> Dict:
        """LightGBM ëª¨ë¸ í•™ìŠµ"""
        if len(features) < 60:
            return {'trained': False, 'message': 'ë°ì´í„° ë¶€ì¡±'}
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        test_size = min(30, int(len(features) * 0.2))
        X_train, X_test = features[:-test_size], features[-test_size:]
        y_train, y_test = target[:-test_size], target[-test_size:]
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë³€ë™ì„± ëŒ€ì‘ ê°•í™”)
        params = {
            'objective': 'regression',
            'metric': 'mae',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_child_samples': 20,
            'lambda_l1': 0.1,
            'lambda_l2': 0.1,
            'min_gain_to_split': 0.1,
            'verbose': -1
        }
        
        # í•™ìŠµ
        train_data = lgb.Dataset(X_train, label=y_train)
        valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
        
        self.model = lgb.train(
            params,
            train_data,
            valid_sets=[valid_data],
            num_boost_round=1000,
            callbacks=[
                lgb.early_stopping(stopping_rounds=50),
                lgb.log_evaluation(period=0)
            ]
        )
        
        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = self.model.predict(X_test, num_iteration=self.model.best_iteration)
        y_pred = np.maximum(y_pred, 0)  # ìŒìˆ˜ ì œê±°
        
        mae = mean_absolute_error(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / np.maximum(y_test, 1))) * 100
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
        self.feature_importance = pd.DataFrame({
            'feature': features.columns,
            'importance': self.model.feature_importance(importance_type='gain')
        }).sort_values('importance', ascending=False)
        
        return {
            'trained': True,
            'performance': {
                'Accuracy(%)': 100 - mape,
                'MAE': mae
            },
            'best_iteration': self.model.best_iteration,
            'feature_importance': self.feature_importance,
            'predictions': y_pred
        }
    
    def predict(self, features: pd.DataFrame) -> np.ndarray:
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        if self.model is None:
            return np.array([])
        
        predictions = self.model.predict(features, num_iteration=self.model.best_iteration)
        return np.maximum(predictions, 0)


class VolatilityAwareEnsemble:
    """
    ë³€ë™ì„±ì„ ê³ ë ¤í•œ ë™ì  ì•™ìƒë¸” ëª¨ë¸
    ë³€ë™ì„±ì´ ë†’ì„ ë•ŒëŠ” LSTMì˜ ê°€ì¤‘ì¹˜ë¥¼ ë†’ì´ê³ ,
    ì•ˆì •ì ì¼ ë•ŒëŠ” ì „í†µì  ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë†’ì…ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.weights = {}
        
    def calculate_dynamic_weights(
        self, 
        volatility_score: float,
        model_performances: Dict[str, float]
    ) -> Dict[str, float]:
        """
        ë³€ë™ì„±ê³¼ ëª¨ë¸ ì„±ëŠ¥ì— ë”°ë¼ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        
        Args:
            volatility_score: 0~1 ì‚¬ì´ì˜ ë³€ë™ì„± ì ìˆ˜ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë³€ë™ì„± ë†’ìŒ)
            model_performances: ê° ëª¨ë¸ì˜ ì •í™•ë„
        """
        if not model_performances:
            return {}
            
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜: ì„±ëŠ¥ ê¸°ë°˜
        total_performance = sum(model_performances.values())
        if total_performance == 0:
             # ëª¨ë“  ëª¨ë¸ì˜ ì„±ëŠ¥ì´ 0ì¼ ê²½ìš°, ê· ë“±í•˜ê²Œ ê°€ì¤‘ì¹˜ ë°°ë¶„
            num_models = len(model_performances)
            return {model: 1.0 / num_models for model in model_performances}

        base_weights = {
            model: perf / total_performance 
            for model, perf in model_performances.items()
        }
        
        # ë³€ë™ì„± ì¡°ì •
        # ë³€ë™ì„±ì´ ë†’ì„ìˆ˜ë¡ LSTM/LightGBMì— ë” ë§ì€ ê°€ì¤‘ì¹˜
        volatility_adjustment = {
            'prophet': 1 - volatility_score * 0.3,  # ë³€ë™ì„± ë†’ì„ìˆ˜ë¡ ê°ì†Œ
            'xgboost': 1 - volatility_score * 0.2,
            'lightgbm': 1 + volatility_score * 0.2,  # ë³€ë™ì„± ë†’ì„ìˆ˜ë¡ ì¦ê°€
            'lstm': 1 + volatility_score * 0.3
        }
        
        # ì¡°ì •ëœ ê°€ì¤‘ì¹˜ ê³„ì‚°
        adjusted_weights = {}
        for model in base_weights:
            if model in volatility_adjustment:
                adjusted_weights[model] = base_weights[model] * volatility_adjustment[model]
            else:
                adjusted_weights[model] = base_weights[model]
        
        # ì •ê·œí™”
        total_weight = sum(adjusted_weights.values())
        if total_weight == 0:
            num_models = len(adjusted_weights)
            return {model: 1.0 / num_models for model in adjusted_weights}
            
        normalized_weights = {
            model: weight / total_weight 
            for model, weight in adjusted_weights.items()
        }
        
        self.weights = normalized_weights
        return normalized_weights
    
    def ensemble_predict(
        self, 
        predictions: Dict[str, np.ndarray],
        volatility_score: float,
        model_performances: Dict[str, float]
    ) -> np.ndarray:
        """
        ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì•™ìƒë¸”
        """
        weights = self.calculate_dynamic_weights(volatility_score, model_performances)
        
        # ê°€ì¤‘ í‰ê· 
        ensemble_pred = np.zeros_like(list(predictions.values())[0])
        for model, pred in predictions.items():
            if model in weights:
                ensemble_pred += pred * weights[model]
        
        return ensemble_pred


# í†µí•© ì‚¬ìš© ì˜ˆì‹œ
def train_advanced_models(df: pd.DataFrame, product_name: str, features: pd.DataFrame, target: pd.Series):
    """
    ì¶”ê°€ ëª¨ë¸ë“¤ì„ í•™ìŠµí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        Dict: ê° ëª¨ë¸ì˜ í•™ìŠµ ê²°ê³¼ ë° ì„±ëŠ¥
    """
    results = {}
    
    # 1. LSTM í•™ìŠµ
    print(f"  ğŸ§  {product_name}: LSTM ëª¨ë¸ í•™ìŠµ ì¤‘...")
    lstm_model = LSTMPredictor(lookback_days=14, forecast_days=7)
    lstm_result = lstm_model.train(df, product_name)
    results['lstm'] = {
        'model': lstm_model,
        'result': lstm_result
    }
    
    # 2. LightGBM í•™ìŠµ
    print(f"  ğŸ’¡ {product_name}: LightGBM ëª¨ë¸ í•™ìŠµ ì¤‘...")
    lgb_model = LightGBMPredictor()
    lgb_result = lgb_model.train(features, target)
    results['lightgbm'] = {
        'model': lgb_model,
        'result': lgb_result
    }
    
    return results